The  Neural  Consensus  Engine:  A  
Comprehensive
 
Architectural
 
and
 
Development
 
Guide
 
for
 
Agentic
 
Mixture-of-Experts
 
Systems
 1.  Introduction:  The  Paradigm  Shift  to  Agentic  
Consensus
 
The  contemporary  landscape  of  Large  Language  Models  (LLMs)  is  undergoing  a  fundamental  
transformation
 
from
 
monolithic,
 
single-shot
 
inference
 
to
 
composite,
 
agentic
 
architectures.
 
While
 
foundational
 
models
 
have
 
demonstrated
 
remarkable
 
generalist
 
capabilities,
 
they
 
remain
 
prone
 
to
 
hallucinations,
 
stochastic
 
variability,
 
and
 
limitations
 
in
 
reasoning
 
depth
 
when
 
constrained
 
to
 
a
 
single
 
"train
 
of
 
thought."
 
The
 
Neural
 
Consensus
 
Engine
 
proposed
 
in
 
this
 
guide
 
represents
 
a
 
sophisticated
 
implementation
 
of
 
a
 
Mixture
 
of
 
Experts
 
(MoE)
 
system
 
operating
 
at
 
the
 
agentic
 
level.
 
Unlike
 
traditional
 
architectural
 
MoE—where
 
routing
 
occurs
 
between
 
feed-forward
 
networks
 
within
 
the
 
model
 
layers—this
 
system
 
externalizes
 
the
 
"experts"
 
as
 
distinct
 
agentic
 
personas
 
orchestrated
 
through
 
a
 
high-performance
 
framework.
 
This  report  details  the  end-to-end  development  of  the  Neural  Consensus  Engine,  a  system  
designed
 
to
 
synthesize
 
high-fidelity
 
textual
 
outputs
 
by
 
aggregating
 
diverse
 
cognitive
 
perspectives.
 
By
 
instantiating
 
multiple
 
agents—specifically
 
utilizing
 
the
 
Google
 
Gemini
 
API
—parameterized
 
with
 
unique
 
personas,
 
temperatures,
 
and
 
reasoning
 
styles,
 
the
 
engine
 
mimics
 
a
 
human
 
expert
 
panel.
 
A
 
central
 
"Synthesizer
 
Agent"
 
then
 
functions
 
as
 
a
 
meta-cognitive
 
adjudicator,
 
resolving
 
conflicts
 
and
 
formatting
 
the
 
final
 
consensus
 
according
 
to
 
user-defined
 
stylistic
 
modes,
 
akin
 
to
 
advanced
 
editorial
 
tools.
 
The  architecture  is  built  upon  a  decoupled  stack:  a  Python  backend  leveraging  FastAPI  for  
high-concurrency
 
asynchronous
 
execution
 
and
 
LangGraph
 
for
 
stateful
 
orchestration,
 
paired
 
with
 
a
 
reactive
 
React
 
frontend
 
for
 
real-time
 
visualization
 
and
 
parameter
 
tuning.
 
This
 
guide
 
provides
 
an
 
exhaustive
 
technical
 
analysis
 
of
 
the
 
implementation
 
strategies,
 
addressing
 
critical
 
challenges
 
such
 
as
 
asynchronous
 
concurrency
 
patterns,
 
secure
 
API
 
key
 
rotation
 
via
 
context
 
variables,
 
and
 
the
 
nuances
 
of
 
prompt
 
engineering
 
for
 
style
 
transfer.
 
2.  Architectural  Foundations  and  Design  Philosophy  2.1  The  Agentic  Mixture  of  Experts  (MoE)  Topology  
The  core  philosophy  of  the  Neural  Consensus  Engine  aligns  with  ensemble  learning  principles,  
where
 
the
 
aggregation
 
of
 
specialized
 
learners
 
often
 
yields
 
superior
 
performance
 
compared
 
to
 
a  single  strong  learner.  In  the  context  of  Generative  AI,  this  translates  to  utilizing  multiple  
"Expert
 
Agents"
 
that
 
analyze
 
a
 
prompt
 
through
 
different
 
cognitive
 
lenses.
1  
Conventional  interactions  with  LLMs  typically  rely  on  a  single  inference  pass.  This  approach  is  
susceptible
 
to
 
the
 
model's
 
inherent
 
biases
 
and
 
the
 
randomness
 
of
 
its
 
decoding
 
strategy.
 
By
 
contrast,
 
the
 
Neural
 
Consensus
 
Engine
 
employs
 
a
 
Map-Reduce
 
cognitive
 
architecture.
3
 The  
"Map"
 
phase
 
involves
 
determining
 
a
 
set
 
of
 
expert
 
personas—for
 
example,
 
a
 
"Creative
 
Divergent
 
Thinker,"
 
a
 
"Formal
 
Logic
 
Analyst,"
 
and
 
a
 
"Domain
 
Skeptic"—and
 
dispatching
 
the
 
user's
 
query
 
to
 
these
 
agents
 
simultaneously.
 
Each
 
agent
 
operates
 
with
 
distinct
 
generation
 
parameters
 
(temperature,
 
top-k)
 
to
 
force
 
the
 
model
 
into
 
different
 
regions
 
of
 
its
 
latent
 
space,
 
ensuring
 
a
 
rich
 
topology
 
of
 
potential
 
responses.
 
The  "Reduce"  phase  is  handled  by  the  Synthesizer  Agent.  This  component  does  not  merely  
concatenate
 
the
 
outputs;
 
it
 
performs
 
a
 
complex
 
reasoning
 
task
 
to
 
identify
 
commonalities,
 
resolve
 
logical
 
contradictions,
 
and
 
merge
 
the
 
stylistic
 
strengths
 
of
 
each
 
expert
 
into
 
a
 
cohesive
 
whole.
 
This
 
mirrors
 
the
 
Supervisor
 
Architecture
 
found
 
in
 
advanced
 
multi-agent
 
systems,
 
where
 
a
 
central
 
node
 
manages
 
the
 
workflow
 
and
 
handoffs
 
between
 
specialized
 
sub-agents.
2  
2.2  System  Stack  and  Technology  Selection  
The  technology  stack  is  rigorously  selected  to  optimize  for  asynchronous  I/O  operations,  
modularity,
 
and
 
real-time
 
user
 
feedback.
 ●  Backend  Runtime:  Python  3.10+  is  the  standard  for  AI  orchestration.  The  core  web  
framework
 
is
 
FastAPI
,
 
chosen
 
specifically
 
for
 
its
 
native
 
support
 
of
 
asynchronous
 
programming
 
(async/await).
 
This
 
is
 
critical
 
for
 
handling
 
parallel
 
LLM
 
requests
 
without
 
blocking
 
the
 
Global
 
Interpreter
 
Lock
 
(GIL),
 
a
 
common
 
bottleneck
 
in
 
synchronous
 
Python
 
frameworks.
5  
●  Orchestration  Layer:  LangGraph  serves  as  the  state  machine  for  managing  the  
multi-agent
 
workflow.
 
Unlike
 
linear
 
chains,
 
LangGraph
 
allows
 
for
 
cyclic
 
graph
 
structures,
 
enabling
 
the
 
Synthesizer
 
Agent
 
to
 
request
 
revisions
 
from
 
experts
 
if
 
the
 
consensus
 
threshold
 
is
 
not
 
met,
 
thus
 
supporting
 
"human-in-the-loop"
 
patterns
 
and
 
complex
 
handoffs.
1  
●  Inference  Engine:  The  Google  Gen  AI  SDK  (google-genai)  connects  to  the  Gemini  API  
(e.g.,
 
Gemini
 
2.0
 
Flash
 
or
 
Pro).
 
This
 
model
 
family
 
is
 
selected
 
for
 
its
 
multimodal
 
capabilities
 
and
 
large
 
context
 
window,
 
which
 
is
 
essential
 
for
 
processing
 
multiple
 
expert
 
outputs
 
simultaneously
 
without
 
losing
 
coherence.
8  
●  Frontend  Interface:  React  with  TypeScript  provides  the  client-side  logic.  It  utilizes  
Server-Sent
 
Events
 
(SSE)
 
to
 
display
 
expert
 
outputs
 
as
 
they
 
stream
 
in
 
parallel,
 
offering
 
users
 
immediate
 
visibility
 
into
 
the
 
system's
 
"thought
 
process"
 
before
 
the
 
final
 
consensus
 
is
 
rendered.
10  
2.3  Data  Flow  and  State  Management  
The  data  flow  within  the  Neural  Consensus  Engine  follows  a  strict  stateful  progression.  A  
shared
 
state
 
object,
 
managed
 
by
 
LangGraph,
 
persists
 
throughout
 
the
 
request
 
lifecycle.
 
This
 
state
 
object
 
aggregates
 
the
 
user's
 
query,
 
the
 
specific
 
parameters
 
for
 
each
 
expert
 
(temperature,
 
persona),
 
the
 
raw
 
outputs
 
from
 
the
 
Map
 
phase,
 
and
 
the
 
final
 
synthesized
 
text.
 
Component  Responsibility  State  Interaction  
API  Gateway  Request  validation,  API  Key  injection  
Initializes  AgentState  
Dispatcher  Node  Spawns  parallel  expert  tasks  
Reads  query,  writes  expert_tasks  
Expert  Nodes  Generate  distinct  responses  
Write  to  expert_outputs  list  
Synthesizer  Node  Aggregates  and  formats  Reads  expert_outputs,  writes  final_consensus  
Judge  Node  Evaluates  quality  (Optional)  Reads  final_consensus,  writes  quality_score  
This  structured  state  management  ensures  that  the  system  is  deterministic  and  debuggable.  If  
a
 
consensus
 
fails,
 
developers
 
can
 
inspect
 
the
 
state
 
at
 
the
 
exact
 
moment
 
of
 
failure
 
to
 
understand
 
which
 
expert
 
introduced
 
the
 
divergence.
7  
3.  Backend  Infrastructure  and  Concurrency  Patterns  
The  backend  serves  as  the  engine  room  of  the  application,  responsible  for  managing  the  
lifecycle
 
of
 
a
 
request
 
from
 
user
 
input
 
to
 
finalized
 
consensus.
 
The
 
primary
 
engineering
 
challenge
 
in
 
a
 
Mixture
 
of
 
Experts
 
system
 
is
 
latency;
 
executing
 
three
 
to
 
five
 
LLM
 
calls
 
sequentially
 
would
 
result
 
in
 
an
 
unacceptable
 
wait
 
time
 
for
 
the
 
end
 
user.
 
Therefore,
 
a
 
robust
 
asynchronous,
 
parallel
 
execution
 
model
 
is
 
non-negotiable.
 
3.1  Asynchronous  Execution  Strategy  with  Asyncio  
To  achieve  the  "Mixture"  effect  efficiently,  the  system  must  query  multiple  experts  
simultaneously.
 
Python's
 
asyncio
 
library
 
is
 
the
 
standard
 
for
 
this
 
I/O-bound
 
task.
 
While
 
Python's
 
Global
 
Interpreter
 
Lock
 
(GIL)
 
often
 
limits
 
CPU-bound
 
concurrency,
 
it
 
is
 
not
 
a
 
bottleneck
 
here
 
because
 
the
 
threads
 
spend
 
the
 
vast
 
majority
 
of
 
their
 
time
 
waiting
 
for
 
the
 
HTTP  response  from  Google's  servers.
5  
The  architectural  pattern  utilizes  asyncio.gather  to  schedule  expert  tasks.  This  function  takes  
a
 
list
 
of
 
awaitable
 
objects
 
(the
 
API
 
calls)
 
and
 
runs
 
them
 
concurrently,
 
returning
 
the
 
results
 
in
 
the
 
order
 
of
 
the
 
input
 
list
 
once
 
all
 
are
 
complete.
 
This
 
is
 
significantly
 
more
 
efficient
 
than
 
threading
 
for
 
high-concurrency
 
network
 
operations
 
because
 
it
 
avoids
 
the
 
overhead
 
of
 
context
 
switching
 
associated
 
with
 
OS
 
threads.
13  
Implementation  of  Parallel  Dispatch  The  ExpertDispatch  service  constructs  the  specific  prompt  for  each  expert  based  on  their  
persona
 
configuration
 
(Temperature,
 
Top-K,
 
System
 
Instruction)
 
and
 
dispatches
 
them.
 
The
 
code
 
must
 
handle
 
the
 
creation
 
of
 
the
 
genai.Client
 
and
 
the
 
specific
 
generation
 
configuration
 
for
 
each
 
call.
 
 
Python  
  import asyncio  from google  import genai  from google.genai  import types  from typing  import List,  Dict  
 async def fetch_expert_response(client:  genai.Client,  model_id:  str,  prompt:  str,  config:  
types.GenerateContentConfig,
 
expert_id:
 str)  ->  Dict: 
    """  
    
Wraps
 
the
 
API
 
call
 
to
 
return
 
identity
 
metadata
 
along
 
with
 
the
 
response.
 
    
This
 
ensures
 
that
 
when
 
results
 
are
 
gathered,
 
we
 
know
 
which
 
expert
 
produced
 
which
 
text.
 
    
""" 
    try:  
        #  Utilizing  the  async  interface  of  the  Google  Gen  AI  SDK 
        
response
 
=
 await client.aio.models.generate_content(  
            
model=model_id,
 
            
contents=prompt,
 
            
config=config
 
        
)
 
        return {  
            "expert_id":  expert_id,  
            "content":  response.text,  
            "status":  "success" 
        
}
 
    except Exception  as e:  
        return {  
            "expert_id":  expert_id,  
            "error":  str(e),  
            "status":  "failed" 
        
}
 
 async def run_mixture_of_experts(query:  str,  experts_config:  List,  api_key:  str): 
    #  Initialize  the  client  with  the  provided  API  key 
    
client
 
=
 
genai.Client(api_key=api_key)
 
    
 
    
tasks
 
=
 
    for expert  in experts_config:  
        #  Configure  the  specific  generation  parameters  for  this  expert 
        
gen_config
 
=
 
types.GenerateContentConfig(
 
            
temperature=expert.get('temperature',  0.7),  
            
top_p=expert.get('top_p',  0.95),  
            
top_k=expert.get('top_k',  40),  
            
system_instruction=expert.get('system_instruction')  
        
)
 
        
 
        #  Schedule  the  task  without  awaiting  it  yet 
        
tasks.append(
 
            
fetch_expert_response(
 
                
client,
 
 
                "gemini-2.0-flash",   
                
query,
 
 
                
gen_config,
 
 
                
expert['name']  
            
)
 
        
)
 
    
 
    #  Execute  all  expert  calls  in  parallel  and  wait  for  all  to  complete 
    
results
 
=
 await asyncio.gather(*tasks)  
    return results  
 This  implementation  demonstrates  the  non-blocking  nature  of  the  engine.  By  iterating  through  
a
 
configuration
 
list
 
of
 
experts—each
 
defining
 
a
 
unique
 
system_instruction
 
representing
 
their
 
persona—we
 
schedule
 
concurrent
 
calls.
 
The
 
client.aio
 
property
 
is
 
the
 
specific
 
asynchronous
 
interface
 
of
 
the
 
Google
 
Gen
 
AI
 
SDK,
 
which
 
is
 
crucial;
 
using
 
the
 
synchronous
 
client.models.generate_content
 
would
 
block
 
the
 
event
 
loop
 
and
 
force
 
sequential
 
execution,
 
defeating
 
the
 
purpose
 
of
 
the
 
architecture.
8  
3.2  Managing  Rate  Limits  with  Semaphores  
While  asyncio.gather  offers  unbounded  concurrency,  real-world  APIs  impose  rate  limits  
(Requests
 
Per
 
Minute
 
-
 
RPM)
 
and
 
token
 
limits
 
(Tokens
 
Per
 
Minute
 
-
 
TPM).
 
Triggering
 
50
 
experts
 
simultaneously
 
would
 
likely
 
result
 
in
 
HTTP
 
429
 
(Too
 
Many
 
Requests)
 
errors,
 
causing
 
the
 
entire
 
request
 
to
 
fail.
 
To
 
handle
 
this
 
robustly,
 
the
 
architecture
 
implements
 
a
 
semaphore
 
pattern.
13  
A  semaphore  acts  as  a  gatekeeper,  allowing  only  a  fixed  number  of  concurrent  operations  to  
proceed
 
while
 
others
 
wait
 
in
 
a
 
queue.
 
This
 
is
 
preferable
 
to
 
simple
 
batching
 
because
 
it
 
maximizes
 
throughput;
 
as
 
soon
 
as
 
one
 
request
 
finishes,
 
another
 
begins,
 
keeping
 
the
 
"pipe"
 
full
 
without
 
overflowing
 
it.
 
 
Python  
  #  Limit  to  5  concurrent  requests  to  respect  standard  API  quotas 
MAX_CONCURRENT_REQUESTS
 
=
 5 
semaphore
 
=
 
asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
 
 async def rate_limited_expert_call(client,  model,  prompt,  config,  expert_id): 
    async with semaphore:  
        return await fetch_expert_response(client,  model,  prompt,  config,  expert_id)  
 Integrating  this  rate_limited_expert_call  into  the  main  dispatch  loop  ensures  the  application  
remains
 
a
 
"polite"
 
client
 
to
 
the
 
Google
 
Gemini
 
API,
 
preventing
 
cascading
 
failures
 
during
 
high-load
 
consensus
 
generation.
 
It
 
smoothens
 
the
 
traffic
 
spikes
 
that
 
naturally
 
occur
 
in
 
fan-out
 
architectures.
 
3.3  Security  and  API  Key  Management  
The  system  relies  heavily  on  the  Google  Gemini  API.  Managing  API  keys  securely  is  vital,  
especially
 
in
 
a
 
production
 
environment
 
where
 
we
 
might
 
want
 
to
 
rotate
 
keys
 
to
 
avoid
 
hitting
 
quotas
 
during
 
high
 
concurrency
 
or
 
to
 
attribute
 
usage
 
to
 
specific
 
tenants.
16  
Key  Rotation  Strategy  Hardcoding  API  keys  is  a  critical  security  vulnerability.  The  Neural  Consensus  Engine  employs  
a
 
rotation
 
strategy
 
using
 
a
 
pool
 
of
 
keys
 
loaded
 
from
 
environment
 
variables.
 
This
 
distributes
 
the
 
load
 
across
 
multiple
 
keys
 
(if
 
permissible
 
by
 
the
 
provider's
 
terms)
 
or
 
ensures
 
high
 
availability
 
if
 
one
 
key
 
is
 
revoked.
 
 
Python  
  import os  import itertools  from contextvars  import ContextVar  
 #  Load  keys  from  environment  variables  (e.g.,  GEMINI_KEY_1,  GEMINI_KEY_2) 
api_keys
 
=
 if not api_keys:  
    raise ValueError("No  API  keys  found  in  environment  variables.")  
 #  Create  an  infinite  iterator  for  round-robin  rotation 
key_cycle
 
=
 
itertools.cycle(api_keys)
 
 def get_next_api_key(): 
    return next(key_cycle)  
 Context-Aware  Key  Injection  via  ContextVars  In  an  asynchronous  context,  ensuring  that  the  correct  API  key  is  used  for  the  duration  of  a  
specific
 
request
 
(especially
 
if
 
we
 
implement
 
"Bring
 
Your
 
Own
 
Key"
 
functionality)
 
requires
 
careful
 
state
 
management.
 
Python's
 
contextvars
 
module
 
is
 
designed
 
for
 
this
 
exact
 
purpose,
 
providing
 
thread-safe
 
and
 
async-safe
 
storage
 
for
 
context-local
 
state.
18  
We  define  a  ContextVar  to  hold  the  API  key  for  the  current  request  context.  This  variable  is  set  
at
 
the
 
beginning
 
of
 
the
 
request
 
(e.g.,
 
in
 
a
 
middleware
 
or
 
dependency)
 
and
 
can
 
be
 
accessed
 
deeply
 
within
 
the
 
nested
 
async
 
calls
 
of
 
the
 
expert
 
dispatchers
 
without
 
passing
 
the
 
key
 
as
 
an
 
argument
 
through
 
every
 
function.
 
 
Python  
  from contextvars  import ContextVar  from fastapi  import Request  
 #  Define  a  context  variable  for  the  API  key 
request_api_key:
 
ContextVar[str]  =  ContextVar("request_api_key",  default=None)  
 
async def get_gemini_client(): 
    """  
    
Dependency
 
to
 
be
 
injected
 
into
 
routes
 
or
 
used
 
by
 
services.
 
    
It
 
prioritizes
 
a
 
key
 
set
 
in
 
the
 
current
 
context
 
(e.g.,
 
from
 
a
 
header).
 
    
If
 
none
 
is
 
set,
 
it
 
falls
 
back
 
to
 
the
 
system
 
rotation.
 
    
""" 
    
key
 
=
 
request_api_key.get()
 
    if not key:  
        
key
 
=
 
get_next_api_key()
 
 
    return genai.Client(api_key=key)  
 In  the  FastAPI  dependency  injection  system,  we  can  verify  and  set  this  key.  If  a  user  provides  
an
 
X-Goog-Api-Key
 
header,
 
that
 
key
 
takes
 
precedence
 
for
 
their
 
request,
 
ensuring
 
tenant
 
isolation.
20  
4.  The  Mixture  of  Experts  (MoE)  Layer  Implementation  
The  "intelligence"  of  the  Neural  Consensus  Engine  resides  not  in  the  model  itself,  but  in  the  
diversity
 
of
 
its
 
experts.
 
If
 
all
 
agents
 
share
 
the
 
same
 
prompt
 
and
 
parameters,
 
the
 
"consensus"
 
is
 
merely
 
redundancy.
 
We
 
must
 
engineer
 
distinct
 
personas
 
that
 
mimic
 
the
 
cognitive
 
diversity
 
of
 
a
 
human
 
expert
 
panel.
 
This
 
section
 
details
 
the
 
prompt
 
engineering
 
and
 
parameterization
 
required
 
to
 
create
 
these
 
distinct
 
voices.
 
4.1  Defining  Expert  Personas  via  System  Instructions  
The  Google  Gen  AI  SDK  allows  for  system_instructions,  which  are  high-level  directives  that  set  
the
 
behavior,
 
tone,
 
and
 
boundaries
 
of
 
the
 
model.
22
 We  define  a  library  of  expert  archetypes  
that
 
users
 
can
 
select.
 
1.  The  Analytical  Skeptic  (Low  Temperature)  This  expert  is  designed  to  minimize  hallucination  and  maximize  logical  rigor.  ●  Role:  To  scrutinize  the  user's  query  for  logical  fallacies  and  provide  a  purely  factual,  dry  
response.
 ●  System  Instruction:  "You  are  a  rigorous  Logic  Analyst.  Your  goal  is  to  deconstruct  the  
user's
 
query,
 
identifying
 
potential
 
ambiguities.
 
Provide
 
a
 
response
 
based
 
strictly
 
on
 
verifiable
 
facts.
 
Avoid
 
emotional
 
language.
 
If
 
a
 
premise
 
is
 
flawed,
 
point
 
it
 
out
 
immediately.
 
Do
 
not
 
speculate."
 ●  Configuration:  ○  Temperature:  0.1  (Favors  the  most  probable  tokens,  reducing  creativity)  ○  Top-P:  0.8  (Restricts  the  token  pool  to  the  top  80%  cumulative  probability)  
2.  The  Creative  Lateral  Thinker  (High  Temperature)  
This  expert  is  designed  to  break  out  of  local  minima  and  find  novel  solutions.  ●  Role:  To  generate  novel  connections,  metaphors,  and  out-of-the-box  solutions.  ●  System  Instruction:  "You  are  a  Visionary  Creative  Consultant.  Your  goal  is  to  explore  the  
'what
 
ifs'
 
of
 
the
 
user's
 
query.
 
Use
 
metaphors,
 
analogies,
 
and
 
narrative
 
structures.
 
Prioritize
 
novelty
 
over
 
convention.
 
Connect
 
disparate
 
concepts
 
that
 
may
 
not
 
seem
 
immediately
 
related."
 ●  Configuration:  ○  Temperature:  0.9  (increases  randomness  and  diversity)  ○  Top-K:  40  (Allows  the  model  to  select  from  a  wider  range  of  lower-probability  tokens)  23  
3.  The  Empathetic  Mediator  (Mid  Temperature)  This  expert  focuses  on  the  user  experience  and  accessibility  of  the  information.  ●  Role:  To  ensure  the  response  is  user-centric,  accessible,  and  balanced.  ●  System  Instruction:  "You  are  a  User  Experience  Advocate.  Your  goal  is  to  explain  
concepts
 
simply
 
and
 
empathetically.
 
Anticipate
 
user
 
frustration
 
or
 
confusion.
 
Ensure
 
the
 
tone
 
is
 
warm
 
and
 
approachable.
 
Translate
 
technical
 
jargon
 
into
 
plain
 
language."
 ●  Configuration:  ○  Temperature:  0.5  (A  balance  between  determinism  and  fluency)  
These  instructions  are  passed  dynamically  to  the  fetch_expert_response  function.  By  varying  
the
 
system_instruction
 
and
 
generation
 
configuration
 
(temperature),
 
we
 
effectively
 
force
 
the
 
underlying
 
Gemini
 
model
 
to
 
adopt
 
different
 
cognitive
 
stances,
 
ensuring
 
the
 
outputs
 
provided
 
to
 
the
 
Synthesizer
 
are
 
distinct
 
and
 
complementary.
24  
4.2  The  Synthesizer  Agent:  Map-Reduce  Logic  
Once  the  experts  (the  "Map"  phase)  have  returned  their  outputs,  the  Synthesizer  Agent  (the  
"Reduce"
 
phase)
 
executes
 
the
 
core
 
value
 
proposition
 
of
 
the
 
system.
 
This
 
process
 
is
 
distinct
 
from
 
simple
 
summarization;
 
it
 
is
 
an
 
active
 
judgment
 
and
 
integration
 
process.
 
The
 
Synthesizer
 
functions
 
like
 
a
 
lead
 
editor
 
who
 
reads
 
three
 
drafts
 
and
 
compiles
 
the
 
final
 
article.
3  
The  prompt  for  the  Synthesizer  is  critical.  It  utilizes  Chain  of  Thought  (CoT)  prompting  to  
ensure
 
high-quality
 
synthesis.
 
Synthesizer  Prompt  Template:  You  are  the  Neural  Consensus  Engine,  a  master  synthesizer  of  information.  You  have  been  provided  with  distinct  perspectives  from  three  expert  agents  regarding  the  
user's
 
query:
 
"{user_query}".
 Expert  A  (Analytic):  {response_A}  Expert  B  (Creative):  {response_B}  Expert  C  (Mediator):  {response_C}  
Your  Task  is  to  create  a  unified  response.  Follow  these  steps:  1.  Analysis :  Identify  the  core  truths  agreed  upon  by  all  experts.  2.  Conflict  Resolution :  Note  meaningful  divergences  or  conflicts.  If  Expert  A  disputes  a  
fact
 
claimed
 
by
 
Expert
 
B,
 
prioritize
 
the
 
evidence
 
provided
 
by
 
Expert
 
A
 
(Analytic).
 3.  Synthesis :  Construct  a  final,  single  coherent  response  that  integrates  the  factual  rigor  of  
A,
 
the
 
innovative
 
framing
 
of
 
B,
 
and
 
the
 
accessibility
 
of
 
C.
 4.  Formatting :  Adhere  strictly  to  the  user's  requested  style  preference:  
"{user_style_preference}"
 
(e.g.,
 
Academic,
 
Casual,
 
Bulleted).
 
Do  not  explicitly  mention  "Expert  A  said  this..."  in  the  final  output  unless  highlighting  a  
specific,
 
unresolved
 
conflict.
 
The
 
final
 
output
 
should
 
read
 
as
 
a
 
unified,
 
authoritative
 
voice.
 
This  prompt  structure  enforces  the  "Consensus"  aspect.  By  explicitly  asking  the  model  to  
identify
 
"core
 
truths"
 
and
 
"divergences,"
 
we
 
implement
 
a
 
form
 
of
 
CoT
 
reasoning
 
within
 
the
 
synthesis
 
step,
 
improving
 
the
 
coherence
 
of
 
the
 
final
 
result.
25  
4.3  Orchestration  with  LangGraph  
While  a  simple  Python  script  could  handle  the  map-reduce  flow,  LangGraph  provides  the  
resilience
 
and
 
state
 
management
 
needed
 
for
 
a
 
production-grade
 
application.
 
It
 
models
 
the
 
conversation
 
as
 
a
 
graph
 
of
 
nodes
 
and
 
edges,
 
allowing
 
for
 
complex
 
control
 
flows.
 
State  Definition:  We  define  a  typed  dictionary  to  represent  the  state  of  the  graph.   
Python  
  from typing  import TypedDict,  Annotated,  List  import operator  
 class AgentState(TypedDict): 
    
query:
 str 
    #  The  operator.add  reducer  allows  multiple  expert  nodes  to  append  to  this  list 
    
expert_outputs:
 
Annotated[List[str],  operator.add]   
    
final_consensus:
 str 
 Graph  Workflow:  1.  Start  Node:  Receives  the  initial  user  query  and  initializes  the  state.  2.  Expert  Node  (Map):  This  node  is  responsible  for  the  parallel  execution.  In  LangGraph,  
we
 
can
 
use
 
the
 
Send
 
API
 
to
 
dynamically
 
spawn
 
workers,
 
or
 
simply
 
have
 
a
 
single
 
node
 
that
 
awaits
 
the
 
asyncio.gather
 
of
 
all
 
experts
 
and
 
updates
 
expert_outputs
 
in
 
the
 
state.
2  
3.  Synthesizer  Node  (Reduce):  This  node  reads  the  expert_outputs  list  from  the  state,  
constructs
 
the
 
synthesis
 
prompt,
 
calls
 
the
 
Gemini
 
model,
 
and
 
updates
 
final_consensus.
 4.  End  Node:  Returns  the  final  result  to  the  API  response  handler.  
The  use  of  LangGraph  allows  for  future  extensibility.  For  instance,  we  could  add  a  "Review"  
cycle
 
where,
 
if
 
the
 
Synthesizer
 
determines
 
the
 
experts
 
disagree
 
too
 
fundamentally
 
(e.g.,
 
low
 
semantic
 
similarity
 
between
 
outputs),
 
the
 
graph
 
could
 
route
 
back
 
to
 
the
 
experts
 
with
 
a
 
refinement
 
prompt
 
("Expert
 
A
 
and
 
B
 
disagreed
 
on
 
X,
 
please
 
clarify
 
X"),
 
creating
 
a
 
multi-turn
 
consensus
 
protocol.
3  
5.  Frontend  Development:  The  Consensus  Dashboard  
The  frontend  serves  as  the  control  center  for  the  Neural  Consensus  Engine.  Unlike  a  standard  
chatbot
 
interface,
 
this
 
dashboard
 
requires
 
controls
 
for
 
configuring
 
the
 
"mixture"
 
and
 
visualizations
 
for
 
the
 
parallel
 
generation
 
process.
 
The
 
frontend
 
is
 
built
 
using
 
React
 
with
 
TypeScript
,
 
leveraging
 
a
 
component
 
library
 
like
 
Shadcn
 
UI
 
for
 
a
 
polished,
 
professional
 
aesthetic.
28  
5.1  Dashboard  Architecture  and  Component  Design  
The  UI  is  divided  into  three  primary  functional  zones:  1.  Configuration  Panel  (Left  Sidebar):  This  area  houses  the  controls  for  the  MoE  
parameters.
 ○  Temperature  Sliders:  Standard  range  sliders  (0.0  -  1.0)  allowing  users  to  adjust  the  
creativity
 
of
 
the
 
experts.
 ○  Top-K  Knobs:  A  specialized  rotary  knob  component  is  used  for  Top-K  (range  1-40)  
to
 
distinguish
 
it
 
visually
 
from
 
the
 
linear
 
sliders.
 
This
 
provides
 
intuitive
 
tactile
 
control
 
over
 
the
 
token
 
selection
 
pool.
30  
○  Persona  Selectors:  Dropdown  menus  allow  users  to  assign  specific  roles  (e.g.,  
"Skeptic",
 
"Creative",
 
"Academic")
 
to
 
Expert
 
Slots
 
1,
 
2,
 
and
 
3.
 ○  Style  Mode  Toggle:  A  segmented  control  allows  users  to  select  the  Synthesizer's  
output
 
style
 
(e.g.,
 
"Formal",
 
"Simple",
 
"Creative"),
 
mimicking
 
the
 
functionality
 
of
 
tools
 
like
 
Quillbot.
32  
2.  Expert  Stream  Grid  (Bottom/Center):  A  dynamic  grid  displaying  the  raw  output  from  
each
 
expert
 
agent
 
in
 
real-time.
 
Each
 
pane
 
in
 
the
 
grid
 
corresponds
 
to
 
an
 
expert
 
(e.g.,
 
"Expert
 
A:
 
Analytic").
 
This
 
transparency
 
is
 
crucial
 
for
 
the
 
user
 
to
 
trust
 
the
 
consensus;
 
they
 
can
 
see
 
the
 
raw
 
materials
 
before
 
they
 
are
 
synthesized.
 3.  Consensus  Editor  (Top/Center):  The  main  view  showing  the  final  synthesized  result.  
This
 
is
 
a
 
read-only
 
text
 
area
 
that
 
updates
 
as
 
the
 
Synthesizer
 
agent
 
produces
 
tokens.
 
5.2  Streaming  Implementation  with  Server-Sent  Events  (SSE)  
Waiting  for  the  entire  process  (Expert  Generation  +  Synthesis)  to  finish  before  showing  any  
output  creates  a  poor  user  experience  due  to  the  compounding  latency.  We  utilize  streaming  
to
 
pipe
 
the
 
expert
 
tokens
 
to
 
the
 
frontend
 
as
 
they
 
are
 
generated.
 
FastAPI  Streaming  Endpoint:  We  use  StreamingResponse  to  push  data  chunks.  Since  we  have  multiple  experts  generating  
data
 
simultaneously,
 
we
 
need
 
to
 
structure
 
the
 
stream
 
to
 
indicate
 
which
 
expert
 
a
 
token
 
belongs
 
to.
 
We
 
define
 
a
 
custom
 
SSE
 
format
 
where
 
each
 
event
 
carries
 
metadata.
  
Python  
  from fastapi.responses  import StreamingResponse  import json  
 async def stream_consensus_workflow(query,  config): 
    #  This  generator  yields  events  from  the  LangGraph  execution 
    async for event  in graph.astream_events(inputs,  version="v1"):  
        if event["event"]  ==  "on_chat_model_stream":  
            #  Identify  which  node  produced  this  token  (Expert  A,  B,  or  Synthesizer) 
            
node_name
 
=
 
event["metadata"].get("langgraph_node")  
            
content
 
=
 
event["data"]["chunk"].content  
            
 
            #  Format  as  SSE  data  payload 
            
payload
 
=
 
json.dumps({"node":  node_name,  "content":  content})  
            yield f"data:  {payload}\n\n" 
 @app.post("/generate/stream") async def generate_stream(request:  ConsensusRequest): 
    return StreamingResponse(  
        
stream_consensus_workflow(request.query,
 
request.config),
 
        
media_type="text/event-stream" 
    
)
 
 React  Consumer:  On  the  frontend,  we  use  the  native  fetch  API  with  a  ReadableStream  reader  to  parse  these  
interleaved
 
streams.
  
TypeScript  
  
const handleStream  =  async ()  =>  {  
  const response  =  await fetch('/api/generate/stream',  {   
    method:  'POST',   
    headers:  {  'Content-Type':  'application/json' },  
    body:  JSON.stringify({  query,  config  })  
  
});
 
  
 
  const reader  =  response.body?.getReader();  
  const decoder  =  new TextDecoder();  
 
  if (!reader)  return;  
 
  while (true)  {  
    const {  done,  value  }  =  await reader.read();  
    if (done)  break;  
    
 
    const chunk  =  decoder.decode(value);  
    const lines  =  chunk.split('\n\n');  
    
 
    
lines.forEach(line  => {  
      if (line.startsWith('data:  '))  {  
        try {  
          const data  =  JSON.parse(line.slice(6));  
          //  Update  state  specific  to  the  node  that  sent  the  token 
          
setExpertOutputs(prev  => ({  
           
...prev,
 
            
[data.node]:
 
(prev[data.node]
 
|
 
 
|
 "")  +  data.content  
          
}));
 
        
}
 catch (e)  {  
          console.error("Error  parsing  SSE  chunk",  e);  
        
}
 
      
}
 
    
});
 
  
}
 
};
 
 This  implementation  allows  the  user  to  watch  Expert  A,  Expert  B,  and  the  Synthesizer  type  out  
their
 
responses
 
simultaneously,
 
providing
 
a
 
futuristic
 
"command
 
center"
 
feel
 
and
 
effectively
 
masking
 
the
 
latency
 
of
 
the
 
backend
 
operations.
11  
6.  Synthesizer  Modes  and  Advanced  Prompt  
Engineering
 
The  value  proposition  of  tools  like  Quillbot  lies  in  their  specific  "modes"  (Standard,  Fluency,  
Creative).
 
We
 
replicate
 
and
 
expand
 
this
 
by
 
mapping
 
these
 
user-facing
 
modes
 
to
 
specific
 
system
 
instructions
 
for
 
the
 
Synthesizer
 
Agent.
 
6.1  Mode  Definitions  and  Prompt  Strategies  
1.  Standard/Consensus  Mode:  ○  Goal:  Balanced,  neutral  synthesis.  ○  Instruction:  "Balance  the  inputs  equally.  Maintain  a  neutral  tone.  Prioritize  clarity  and  
factual
 
agreement.
 
Resolve
 
minor
 
conflicts
 
by
 
deferring
 
to
 
the
 
majority
 
view."
 2.  Creative/Expansion  Mode:  ○  Goal:  Novelty  and  richness.  ○  Instruction:  "Prioritize  the  most  novel  and  divergent  ideas  from  the  experts.  If  Expert  
B
 
suggested
 
a
 
metaphor,
 
expand
 
on
 
it.
 
Use
 
descriptive
 
adjectives
 
and
 
varied
 
sentence
 
structures.
 
Avoid
 
generic
 
phrasing.".
35  
3.  Formal/Academic  Mode:  ○  Goal:  Rigor  and  precision.  ○  Instruction:  "Adopt  a  scholarly  tone.  Synthesize  the  expert  inputs  into  a  formal  
argument.
 
Use
 
precise
 
terminology.
 
Discard
 
colloquialisms
 
found
 
in
 
the
 
expert
 
drafts.
 
Ensure
 
all
 
claims
 
are
 
qualified
 
appropriately.".
36  
4.  Simple/ELI5  (Explain  Like  I'm  5)  Mode:  ○  Goal:  Accessibility.  ○  Instruction:  "Rewrite  the  consensus  for  a  5th-grade  reading  level.  Remove  jargon.  
Use
 
short
 
sentences
 
(maximum
 
15
 
words).
 
Focus
 
on
 
the
 
core
 
'Mediator'
 
expert's
 
input.".
37  
6.2  Implementation  of  Style  Switching  
The  frontend  sends  a  mode  string  (e.g.,  "formal")  to  the  backend.  The  backend  maps  this  
string
 
to
 
a
 
pre-defined
 
prompt
 
template
 
stored
 
in
 
a
 
dictionary.
 
This
 
separation
 
of
 
concerns
 
ensures
 
that
 
prompt
 
engineers
 
can
 
iterate
 
on
 
the
 
style
 
definitions
 
without
 
requiring
 
changes
 
to
 
the
 
core
 
application
 
logic.
 
 
Python  
  MODE_PROMPTS  =  {  
    "formal":  "Adopt  a  scholarly  tone.  Synthesize  the  expert  inputs...",  
    "creative":  "Prioritize  novel  ideas.  Use  descriptive  adjectives...",  
    "simple":  "Rewrite  the  consensus  for  a  5th-grade  reading  level..." 
}
 
 #  In  Synthesizer  Logic  Node 
synthesis_instruction
 
=
 
MODE_PROMPTS.get(user_request.mode,
 
MODE_PROMPTS["standard"])  
 
7.  Advanced  Optimization:  LLM-as-a-Judge  
To  ensure  the  "Neural  Consensus"  is  genuinely  an  improvement  over  a  single  call,  we  
implement
 
an
 
automated
 
evaluation
 
loop
 
using
 
the
 
LLM-as-a-Judge
 
pattern.
 
This
 
adds
 
a
 
layer
 
of
 
quality
 
assurance
 
before
 
the
 
user
 
sees
 
the
 
final
 
result
 
(or
 
as
 
a
 
post-generation
 
metric).
38  
7.1  The  Judge  Agent  
A  separate,  highly  capable  model  instance  (e.g.,  Gemini  1.5  Pro)  acts  as  a  quality  assurance  
auditor.
 
After
 
the
 
Synthesizer
 
produces
 
the
 
draft,
 
the
 
Judge
 
evaluates
 
it
 
against
 
the
 
original
 
expert
 
outputs.
 
Judge  Prompt:  "You  are  a  Quality  Assurance  Auditor.  Review  the  Final  Consensus  against  Expert  A,  B,  and  C.  1.  Hallucination  Check :  Did  the  consensus  hallucinate  new  information  not  present  in  the  
expert
 
outputs?
 2.  Safety  Check :  Did  it  miss  a  critical  warning  from  the  Skeptic  Expert?  3.  Coherence  Score :  Rate  the  synthesis  quality  on  a  scale  of  1-5."  
If  the  score  is  below  a  threshold  (e.g.,  4/5),  the  system  (via  LangGraph)  can  trigger  a  "Repair"  
loop
,
 
feeding
 
the
 
Judge's
 
critique
 
back
 
to
 
the
 
Synthesizer
 
for
 
a
 
second
 
pass.
 
This
 
ensures
 
that
 
the
 
system
 
self-corrects
 
and
 
improves
 
over
 
time,
 
maintaining
 
high
 
standards
 
of
 
reliability.
 
8.  Deployment  and  Scalability  Considerations  
For  production  deployment,  the  Python  backend  is  containerized  using  Docker .  The  
application
 
is
 
stateless,
 
allowing
 
for
 
horizontal
 
scaling
 
on
 
serverless
 
platforms
 
like
 
Google
 
Cloud
 
Run
.
 
To
 
manage
 
state
 
across
 
distributed
 
instances
 
(if
 
long-running
 
persistence
 
is
 
needed
 
beyond
 
the
 
request
 
lifecycle),
 
Redis
 
can
 
be
 
integrated
 
as
 
the
 
backing
 
store
 
for
 
LangGraph
 
checkpoints.
 
API  quotas  are  a  significant  consideration.  The  key  rotation  strategy  implemented  in  the  
backend
 
allows
 
the
 
application
 
to
 
utilize
 
multiple
 
API
 
keys,
 
effectively
 
pooling
 
quotas
 
to
 
handle
 
higher
 
concurrency.
 
However,
 
strict
 
monitoring
 
via
 
tools
 
like
 
LangSmith
 
is
 
recommended
 
to
 
track
 
token
 
usage
 
and
 
latency
 
per
 
expert,
 
enabling
 
further
 
optimization
 
of
 
the
 
system
 
instructions  and  parameter  settings.
2  
9.  Conclusion  
The  Neural  Consensus  Engine  represents  a  significant  leap  from  simple  chatbot  interactions.  
By
 
architecting
 
a
 
system
 
that
 
actively
 
creates,
 
processes,
 
and
 
synthesizes
 
diverse
 
cognitive
 
streams,
 
we
 
build
 
a
 
tool
 
that
 
mimics
 
human
 
collaborative
 
intelligence.
 
The
 
combination
 
of
 
FastAPI's
 
async
 
capabilities,
 
LangGraph's
 
orchestration
 
power,
 
and
 
the
 
Google
 
Gen
 
AI
 
SDK's
 
multimodal
 
expert
 
models
 
creates
 
a
 
robust
 
platform
 
for
 
generating
 
high-quality,
 
nuanced
 
content.
 
This
 
architecture
 
not
 
only
 
mitigates
 
the
 
risk
 
of
 
individual
 
model
 
hallucinations
 
but
 
also
 
provides
 
users
 
with
 
a
 
transparent,
 
configurable,
 
and
 
highly
 
personalized
 
generation
 
experience,
 
setting
 
a
 
new
 
standard
 
for
 
agentic
 
AI
 
applications.
 
Works  cited  
1.  Multi-agent  -  Docs  by  LangChain,  accessed  on  December  18,  2025,  https://docs.langchain.com/oss/python/langchain/multi-agent 2.  LangGraph  Multi-Agent  Systems:  Complete  Tutorial  &  Examples,  accessed  on  
December
 
18,
 
2025,
 https://latenode.com/blog/ai-frameworks-technical-infrastructure/langgraph-multi-agent-orchestration/langgraph-multi-agent-systems-complete-tutorial-examples 3.  Python  LangChain  Course  Summarizing  Long  Texts  Using  ...,  accessed  on  
December
 
18,
 
2025,
 https://academy.finxter.com/python-langchain-course-%F0%9F%90%8D%F0%9F%A6%9C%F0%9F%94%97-summarizing-long-texts-using-langchain-1-6/ 4.  How  to  use  LangChain  and  MapReduce  to  Classify  Text  Using  an  LLM,  accessed  
on
 
December
 
18,
 
2025,
 https://www.bluelabellabs.com/blog/how-to-use-langchain-and-mapreduce/ 5.  Run  multiple  parallel  API  requests  to  LLM  APIs  without  freezing  your  ...,  accessed  
on
 
December
 
18,
 
2025,
 https://medium.com/@ghaelen.m/how-to-run-multiple-parallel-api-requests-to-llm-apis-without-freezing-your-cpu-in-python-asyncio-af0da7e240e3 6.  Make  FastAPI  run  calls  in  parallel  instead  of  serial  -  Sentry,  accessed  on  December  
18,
 
2025,
 https://sentry.io/answers/make-fastapi-run-calls-in-parallel-instead-of-serial/ 7.  Build  multi-agent  systems  with  LangGraph  and  Amazon  Bedrock,  accessed  on  
December
 
18,
 
2025,
 https://aws.amazon.com/blogs/machine-learning/build-multi-agent-systems-with-langgraph-and-amazon-bedrock/ 8.  Google  Gen  AI  Python  SDK  provides  an  interface  for  ...  -  GitHub,  accessed  on  December  18,  2025,  https://github.com/googleapis/python-genai 9.  Google's  Unified  Gen  AI  SDK:  A  Hands-on  Guide  -  DEV  Community,  accessed  on  
December
 
18,
 
2025,
 https://dev.to/ranand12/googles-unified-genai-sdk-a-hands-on-guide-2n2d 
10.  Custom  Response  -  HTML,  Stream,  File,  others  -  FastAPI,  accessed  on  December  18,  2025,  https://fastapi.tiangolo.com/advanced/custom-response/ 11.  How  to  Build  a  Streaming  Agent  with  Burr,  FastAPI,  and  React,  accessed  on  
December
 
18,
 
2025,
 https://medium.com/data-science/how-to-build-a-streaming-agent-with-burr-fastapi-and-react-e2459ef527a8 12.  Scaling  LLM  Calls  Efficiently  in  Python:  The  Power  of  asyncio,  accessed  on  
December
 
18,
 
2025,
 https://medium.com/@kannappansuresh99/scaling-llm-calls-efficiently-in-python-the-power-of-asyncio-bfa969eed718 13.  Python  Asyncio  for  LLM  Concurrency:  Best  Practices  -  Newline.co,  accessed  on  
December
 
18,
 
2025,
 https://www.newline.co/@zaoyang/python-asyncio-for-llm-concurrency-best-practices--bc079176 14.  Parallel  LLM  Calls  from  Scratch  —  Tutorial  For  Dummies  (Using  ...,  accessed  on  
December
 
18,
 
2025,
 https://dev.to/zachary62/parallel-llm-calls-from-scratch-tutorial-for-dummies-using-pocketflow-1972 15.  Submodules  -  Google  Gen  AI  SDK  documentation,  accessed  on  December  18,  2025,  https://googleapis.github.io/python-genai/genai.html 16.  API  and  API  Design:  API  Keys  &  Management  -  Backend  Weekly,  accessed  on  
December
 
18,
 
2025,
 https://newsletter.masteringbackend.com/p/api-and-api-design-api-keys-management 17.  Using  Gemini  API  keys  |  Google  AI  for  Developers,  accessed  on  December  18,  2025,  https://ai.google.dev/gemini-api/docs/api-key 18.  async  version  of  Context.run  for  context  vars  in  python  asyncio?,  accessed  on  
December
 
18,
 
2025,
 https://stackoverflow.com/questions/78659844/async-version-of-context-run-for-context-vars-in-python-asyncio 19.  contextvars  |  Python  Standard  Library,  accessed  on  December  18,  2025,  https://realpython.com/ref/stdlib/contextvars/ 20.  Usage  of  ContextVar  in  Fastapi  #8628  -  GitHub,  accessed  on  December  18,  2025,  https://github.com/fastapi/fastapi/discussions/8628 21.  Dependencies  -  FastAPI,  accessed  on  December  18,  2025,  https://fastapi.tiangolo.com/tutorial/dependencies/ 22.  Use  system  instructions  |  Generative  AI  on  Vertex  AI,  accessed  on  December  18,  
2025,
 https://docs.cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions 23.  React  Slider  component  -  Material  UI,  accessed  on  December  18,  2025,  https://mui.com/material-ui/react-slider/ 24.  Prompt  design  strategies  |  Gemini  API  |  Google  AI  for  Developers,  accessed  on  December  18,  2025,  https://ai.google.dev/gemini-api/docs/prompting-strategies 25.  What  is  chain  of  thought  (CoT)  prompting?  -  IBM,  accessed  on  December  18,  
2025,  https://www.ibm.com/think/topics/chain-of-thoughts 26.  Chain  of  Thought  Prompting:  A  Guide  to  Enhanced  AI  Reasoning,  accessed  on  
December
 
18,
 
2025,
 https://www.openxcell.com/blog/chain-of-thought-prompting/ 27.  The  “LLM  as  a  Judge”  Pattern  -  by  Gareth  Hallberg  -  Medium,  accessed  on  
December
 
18,
 
2025,
 https://medium.com/@gareth.hallberg_55290/the-llm-as-a-judge-pattern-5de93a1e0fa1 28.  Playground  -  Shadcn  UI,  accessed  on  December  18,  2025,  https://ui.shadcn.com/examples/playground 29.  Introduction  -  AI  SDK,  accessed  on  December  18,  2025,  https://ai-sdk.dev/elements 30.  React:  Create  a  turnable  knob  component  -  DEV  Community,  accessed  on  
December
 
18,
 
2025,
 https://dev.to/syeo66/react-create-a-turnable-knob-component-5c85 31.    Unstyled  &  accessible  knob  primitive  for  React.  :  r/reactjs  -  Reddit,  accessed  on  
December
 
18,
 
2025,
 https://www.reddit.com/r/reactjs/comments/1775i7h/reactknobheadless_unstyled_accessible_knob/ 32.  Google  Gemini  Prompts  With  Ready-to-Use  Examples  -  QuillBot,  accessed  on  
December
 
18,
 
2025,
 https://quillbot.com/blog/ai-prompt-writing/prompts-for-gemini/ 33.  (PDF)  Supporting  L2  Writing  Using  QuillBot  -  ResearchGate,  accessed  on  
December
 
18,
 
2025,
 https://www.researchgate.net/publication/380745555_Supporting_L2_Writing_Using_QuillBot 34.  Streaming  APIs  with  FastAPI  and  Next.js  —  Part  2  -  DEV  Community,  accessed  on  
December
 
18,
 
2025,
 https://dev.to/sahan/streaming-apis-with-fastapi-and-nextjs-part-2-2jof 35.  QuillBot:  Revolutionizing  writing  clarity  through  AI  |  by  Botsfortomorrow,  
accessed
 
on
 
December
 
18,
 
2025,
 https://medium.com/@botsfortomorrow/quillbot-revolutionizing-writing-clarity-through-5597e01fcd8a 36.  Constrain  AI  to  Copy  Your  Writing  Style,  Essential  Prompt  Engineering,  accessed  
on
 
December
 
18,
 
2025,
 https://aimclear.com/how-savvy-prompt-engineers-easily-train-ai-to-simulate-your-writing-style/ 37.  Prompt  engineering  techniques  -  Azure  OpenAI  |  Microsoft  Learn,  accessed  on  
December
 
18,
 
2025,
 https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering?view=foundry-classic 38.  LLM-as-a-judge:  a  complete  guide  to  using  LLMs  for  evaluations,  accessed  on  December  18,  2025,  https://www.evidentlyai.com/llm-guide/llm-as-a-judge 39.  LLM  As  a  Judge:  Tutorial  and  Best  Practices  -  Patronus  AI,  accessed  on  December  18,  2025,  https://www.patronus.ai/llm-testing/llm-as-a-judge 
